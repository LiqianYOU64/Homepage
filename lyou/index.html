<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Liqian YOU</title>
    <link href="https://fonts.googleapis.com/css?family=Montserrat&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <script src="jquery-1.8.3.min.js"></script>
</head>

<body>
    <section id="main">
        <nav id="menu" class="fn-clear">

            <ul class='fl'>
                <li class="fl"><a class="fl" href="http://www.youliqian.com.s3-website-ap-southeast-2.amazonaws.com">Liqian's Portfolio</a></li>
                <li class="fl"><a href="javascript:void(0);" onclick="move(1)">Concept</a></li>
                <li class="fl"><a href="javascript:void(0);" onclick="move(2)">Function</a></li>
                <li class="fl"><a href="javascript:void(0);" onclick="move(3)">Design Process</a></li>
                <li class="fl"><a href="javascript:void(0);" onclick="move(4)">Outcome</a></li>
            </ul>
        </nav>

        <header>
            <h1>MoodBall</h1>
            <p>A MUSIC SHARING & EMOTION DETECTION BALL<br>THAT CAN COMMUNICATE MOODS THROUGH MUSIC AND
                LIGHTS<br><br>TEAM POP5
            </p>
            <a href="javascript:void(0);" onclick="move(1)"><img src="image/down.png"></a>
        </header>
    </section>

    <section id="concept">
        <h2>-- Concept & Experience --</h2>
        <img src="image/ball.png">
        <P>MoodBall is the team project of the UQ interaction design program. It is a magic ball that allows
            users to detect and share feelings through music, lights and visualizations. We want to create immersive
            music modulating experience for joy and relaxation as well as to provide mental support through sharing and
            engaging with remote users. <br><br>
            INTERACTION PARADIGM: MoodBall is a Ubiquitous Computing device combined computational capability of
            emotion detection with a series of sensors. It provides a novel approach to mood music sharing,
            which can be used anywhere, anytime.
            <br><br>
            INTERACTION FLOW: Users can touch, squeeze, rotate, press, or shake the ball. The ball can receive a
            user’s inputs and analyse emotions. Then, it will play relevant songs with lights and visualizations
            (projector) and it can also change tempo, pitch and melodies of the music and lights accordingly.
            Users can share music automatically with a remote ball and the remote ball can play the same music with
            lights feedback.
            <br><br>
            FUTURE MUNDANE: From the record, disc, tape, iPod, to smart devices, the way of music sharing has
            been shifted from physically to virtually. Voice-controlled speakers have brought screen-free
            interaction. Nowadays, high technologies such as AI, machine learning and deep learning help us identify the
            emotions brought by different music and play our favorite music at the proper time. People
            can share feelings effectively and efficiently through music by using these novel technologies.
            Future music player tends to be immersive and intelligent. Hope MoodBall can help those who have
            suffered more mental pressures in such a fast-paced work environment to bond social closeness
            physically and remotely, as well as connect with their families and friends through music in
            everyday life.
        </P>
        <div class="conbox">
            <div class="fn-clear btnbox">
                <a href="javascript:;" class="btns fl" onclick="myFunction1()">Show more</a>
                <a href="https://ixd-exhibit.uqcloud.net/pop5/" class="btns fl">Project Web</a>

            </div>
        </div>

        <div id="myDIV1" class="fn-clear" style="width: 100%;display: none;">
            <div id="concept1">
                <img src="image/ball1.jpg">
                <p>INSPIRATION: Our concept is generated from the theme of modulating music. We aim to create a novel
                    way of music sharing which is physical and touchable for everyone. MoodBall is such an approach or
                    we can say a music sharing instrument, providing a collaborative music playing experience. In the
                    beginning, we decided to build a musical instrument public in a small area to a small group. We
                    consider it is like a crystal ball connected with a speaker that allows people to share their
                    favourite songs with others. Users can share songs through the crystal ball and touch the ball to
                    get light feedback. The music will change notes, rhythm, or shift from monochord to chord through
                    physical
                    interaction. We tend to make two balls so that remote users can touch the ball and get music, lights
                    and visual feedback simultaneously.
                </p>
            </div>
            <div id="concept1">
                <img src="image/ball2.jpg">
                <p>SOLUTIONS: The final product of the MoodBall adds emotion detection techniques. It is not merely a
                    modulating music installation, but an emotion sharing device. We built the mood detection system to
                    collect user’s emotion data through various interactions, playing relevant music and getting visual
                    feedback, so as to share feelings through music, lights and visualizations (projector)). We choose
                    to express
                    feelings in a more subtle way -- through music, instead of language, because music is another
                    language of emotion, representing different feelings and barges into the soul without limits or
                    restrictions. Particularly, introverts are usually ashamed to express themselves through words, but
                    music is a good way of imitating their emotions. They can tell a story and share how they “really
                    feel” through music, which allows more people to “understand them”. Meanwhile, according to the user
                    evaluation, we limited our domain space from public to personal places, such as households, and
                    limited our target users as families, friends, and other people with close relationships.
                </p>
            </div>
            <p>
                FINAL EXHIBIT: The exhibit was a great success, and all the sensors worked perfectly. As an external
                student, I felt so pity that I haven’t joined the construction but did some preparation and
                multimedia support. On 27 May, I have attended the online exhibit. Due to the sunshine brightness, the
                lights and visual feedback didn’t display as
                clearly as we expected, but the projector provided an extraordinary visualization at night. We
                received several valuable feedback and responses during the exhibit. Users were attracted by the
                interaction and feedback of the ball. They were desired to know what the present emotions they were
                when touching (pressing or rotating) the ball and surprised at the music, lights and visual
                feedback. What far beyond our expectation is that MoodBall brought users an amazing immersive
                experience, which is popular among all generations, particularly for children. However, there were
                still some limitations in the accuracy of emotion detection functions. Children and young adults can
                not specifically trigger the calm mode and get the correct feedback, because of less pressure strength
                when
                interacting with the ball. The emotion detection system would distinguish between calm and happy
                moods. This response helps us deeply considering our potential users and, in the future, we need to
                do more investigations on users’ behaviour from different aged groups.
            </p>
            <img style="width:100%;" src="image/exhibit.jpg">
            <p> Due to the time limitation, we haven’t built many remote functions so that the remote ball is not as
                attractive as the MoodBall. It only displayed the same music and lights feedback. For the next
                iteration, we may focus on users’ comfort and emotion communication. Meanwhile, users can upload
                their favourite songs through touching, rotating, pressing, etc…The material of the ball should be
                softer and more flexible in order to add more interactions, such as throw the ball when users are
                angry. We hope MoodBall will become a future mundane that would be popular in everyday life.
            </p>
        </div>
        </div>
    </section>
    <section id="function">
        <h2>-- Form & Function --</h2>
        <p>Physical & Playful: Interaction with various ways, including touching, shaking, pressing, rotating,
            moving<br><br>
            Emotion Detection: Detect users moods based on the user's physical interactions in three parameters (GSR
            sensor, pressure sensor and accelerometer gyroscope)
            <br><br>
            Mood music: Interact with the ball and play relevant songs based on the user's emotions<br><br>
            Modulating Music: Emotion detection system collect data from users interaction and different sensors' values
            change tempo and melody of the music<br><br>
            Share emotion: Send this song (with user's current emotion) to a remote ball and both balls can get the same
            audio and visual feedback
            <br><br>
            Simultaneous feedback: Promptly receive other’s emotion through music, lights and visual feedback,
            communicate emotions between sharer and listeners anywhere/anytime
        </p>
    </section>

    <section id="design">
        <div class="fn-clear">
            <h2>-- Design Process --</h2>
            <img class="img1" src="image/anydoor.jpg">
            <div id="depo">
                <h3>Project Inspiration</h3>
                <p>The beginning of design exploration was related to the MoodBall project, which focused on physical
                    computing experience in the aspect of Future Mundane: Novel Interactions for Near-Future
                    Technologies.
                    What I explored in the inspiration presentation was about mind detection, gestures, and movement
                    interaction in the virtual voyage. Inspired by the cartoon Dokodemo, my concept was about an
                    “Anywhere Door” with a brain sensor and a 3D projector, that can bring people to anywhere they want.
                    While this was the initial idea without considering implementation and construction, it was just the
                    reflection of my interest space for further exploration this semester.
                </p>
                <div class="conbox">
                    <div class="fn-clear btnbox center">
                        <a href="image/ProjectInspiration_Liqian.YOU.pdf" class="btns">Link to
                            Poster</a>
                    </div>
                </div>
            </div>
        </div>

        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div id="design" class="fn-clear">
            <img class="img1" src="image/worldcafe.png" alt="">
            <div id="depo">
                <h3>Idea Generation</h3>
                <p>World café helped to generate our previous inspirations into various themes. While my first and
                    second choice was virtual voyage and emotional expression, there were only four students interested
                    in the first theme and many barriers would occur in the product construction for external students.
                    Therefore, I was selected for the theme of modulating music. One interesting feature is our final
                    product combined emotional expression with modulating music. More details can be found in <a
                        href="https://miro.com/app/board/o9J_lTbYgks=/">Journal</a></p>
            </div>
        </div>


        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div class="fn-clear">
            <img class="img1" src="image/11.gif">
            <div id="depo">
                <h3>Proposal Pitch</h3>
                <p>The proposal concept was a music-sharing device called CONNECTION that aims to bond social connection
                    between
                    individuals by allowing them to communicate with each other through music. <br><br>

                    1. A modulating music installation (crystal ball)<br>
                    2. With playful interaction that people can play together<br>
                    3. Public in a small group<br>
                    4. Two or more participants</p>
                <div class="conbox">
                    <div class="fn-clear btnbox">
                        <a href="javascript:;" class="btns fl" onclick="myFunction2()">Show more</a>
                        <a href="https://docs.google.com/presentation/d/1TmR8PBQVFfkUNJCofkQhOwgGwuIB6VWilM86R10gOng/edit?usp=sharing"
                            class="btns fl">Link to
                            Slides</a>

                    </div>
                </div>
            </div>
        </div>

        <div id="myDIV2" style="display: none;">
            <li class="line"><span
                    class="div">________________________________________________________________________</span></li>

            <div class="cour">
                <h4>Quantitative Research</h4>
                <p>In the early stage, we were confused about our domain space and potentials users. Our concept was a
                    little bit broad that we need more investigation. As a UX designer in our team, I played the role of
                    multimedia support and documentation. I prepared many sample materials including videos and
                    websites and analysed domain insights through brainstorming. Here is the
                    affinity diagram:</p>

                <img class="img" style="width: 100%" src="image/affinity.png">

                <p><br>I also conducted an online survey to further explore users behaviour on music sharing, identify
                    potential users and investigate their favourite music categories, experience of listening to music
                    and attitudes towords music and emotion sharing.
                </p>
                <p class="i"><i>“27 responses in 2 days from Wechat group and Facebook group (Uni-students, friends,
                        families and music lovers)”
                    </i></p>
                <p>
                    Based on the feedback, we changed the context of use to the public in a small group and added the
                    music sharing function that user can share their favourite music on the ball. Besides, remote users
                    can receive music feedback and join in modulating the music.
                </p>
                <a
                    href="https://docs.google.com/forms/d/e/1FAIpQLSf6YjWdU3bkb_9b1s4X0nE1e0jEG27CjXjSk5PtmphAK9L2Rw/viewform"><button>Link
                        to Questionnaire</button></a>
            </div>
        </div>

        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div class="fn-clear">
            <a href="image/1.jpg"><img class="img1" src="image/1.jpg"></a>
            <div id="depo">
                <h3>Background Research</h3>
                <p>After proposal presentation, we went through the background research. In this process, our team
                    conducted literature review for preparing conceptual foundation and technical support and
                    qualitative research, including semi-structured interview for open-ended data collection, exploring
                    participant thoughts, feelings and beliefs on emotion sharing through music.
                </p>
                <div class="conbox">
                    <div class="fn-clear btnbox">
                        <a href="javascript:;" class="btns fl" onclick="myFunction3()">Show more</a>
                        <a href="image/proposal_TeamPop5.pdf" class="btns fl">Link to
                            Report</a>

                    </div>
                </div>
            </div>
        </div>

        <div id="myDIV3" style="display: none;">
            <li class="line"><span
                    class="div">________________________________________________________________________</span></li>

            <div class="cour">
                <h4>Literature Review</h4>
                <p>This research method helped us exploring novel technologies with respect to music communicating
                    feelings and emotional tracking. Inspired by Cespedes-Guevara[1], the theory of "music communicates
                    basic emotions as well as constructivist account", we focused on exploring how music expresses core
                    emotions and how to influence listeners' mental status. Following the concept of the perspective of
                    music expression and emotion detection from Maestri’ research[2], we proposed to analyze human-based
                    physical interaction from music and visualization perspectives. The several technologies of music
                    emotion tracking and detection promoted by Lie Lu[3] provide technical support for building emotion
                    detection function in our project.
                </p>
                <p>
                    Reference
                    <br>
                    [1]Cespedes-Guevara, J., & Eerola, T. (2018). Music Communicates Affects, Not Basic Emotions – A
                    Constructionist Account of Attribution of Emotional Meanings to Music. Frontiers in Psychology, 9,
                    215. https://doi.org/10.3389/fpsyg.2018.00215
                    <br>
                    [2]Maestri, E. (2017). A Typo-morphological Approach to Human–Machine Interaction Analysis in Music.
                    Organised Sound, 22(3), 315–323. https://doi.org/10.1017/S1355771817000474
                    <br>
                    [3]Lie Lu, Liu, D., & Hong-Jiang Zhang. (2006). Automatic mood detection and tracking of music audio
                    signals. IEEE Transactions on Audio, Speech and Language Processing, 14(1), 5–18.
                    https://doi.org/10.1109/TSA.2005.860344
                </p>
            </div>

            <li class="line"><span
                    class="div">________________________________________________________________________</span></li>
            <div class="pa">
                <img src="image/interview.jpg" alt="">
                <h4>Semi-structured Interview</h4>
                <p>Following the previous research, I conducted interview to further explore the relationship between
                    music sharing and emotion communication, and the reason for sharing emotions with others. Two
                    participants were selected from the previous research. According to the responses, we limited domain
                    space from public to personal places and narrowed down potential users from strangers to families,
                    friends and those with close relationships. These data helped us to further explore our problem
                    space in the next design stage.
                </p>
                <a href="image/Interview Transcript.pdf"><button>Link
                        to Interview Transcript</button></a>
            </div>
        </div>

        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div class="fn-clear">
            <iframe width="460px" height="280px" src="https://www.youtube.com/embed/bwEPWWMTuLs"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            <div id="depo">
                <h3>Digital Prototype</h3>
                <p>This stage was to build the elementary function of the emotion detection system with music and lights
                    feedback for user evaluation. The prototype was a combination of speaker, LED lights, pressure
                    sensor and accelerometer gyroscope, installing in a transparent plastic ball. Due to the
                    time limitation, remote interaction would be built in the next iteration. Here, I was responsible
                    for video editing and documentation.
                </p>
                <div class="conbox">
                    <div class="fn-clear btnbox">
                        <a href="javascript:;" class="btns fl" onclick="myFunction4()">Show more</a>
                        <a href="image/presentation.pdf" class="btns fl">Link to
                            Presentation</a>
                    </div>
                </div>
            </div>
        </div>

        <div id="myDIV4" style="display: none;">
            <li class="line1"><span
                    class="div">________________________________________________________________________</span></li>

            <div class="pa">
                <img src="image/1.gif" alt="">
                <h4>Interaction Flow</h4>
                <p>1. Detect the user's emotion and play the corresponding music and lights<br>
                    User A touch the Ball: the emotion detection sensor is working, the ball play relevant songs based
                    on the mood after receiving user’s emotion data<br>
                    User A squeeze the Ball: the emotion detection sensor will collect users interaction data and change
                    related mood music
                    <br>
                    <br>
                    2. Remote ball receive the music and lights data simultaneously<br>
                    User B: receive data from User A and play the same music with lights feedback
                </p>
            </div>

            <li class="line1"><span
                    class="div">________________________________________________________________________</span></li>

            <div class="pa">
                <a href="image/ra.png"><img src="image/ra.png" alt=""></a>
                <h4>Form & Function</h4>
                <p>• Speaker: Play various types of songs based on the user’s physical interaction.<br>
                    • LED Light: The LED lights on strips change colour and flicker frequency with the vibration of
                    music.<br>
                    • Pressure Sensor: Users can interact with the ball through squeezing, touching, shaking, pressing
                    and rotating.<br>
                    • GSR Sensor: Measure the electrical conductance of the skin to detect users’ heart rate.<br>
                    • Accelerometer Gyroscope: Measures the vibration and acceleration of motion in X, Y and Z axes to
                    detect users’ movement.
                </p>
            </div>
        </div>

        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div class="fn-clear">
            <a href="image/2.png"><img class="img1" src="image/2.png"></a>
            <div id="depo">
                <h3>User Evaluation</h3>
                <p>In this stage, we conducted bodystroming to evaluate the main functions and interaction flows,
                    collect information on users requirements, and obtain feedback on functions for improvement of the
                    next iteration. According to the feedback, we found various interactions represent different
                    emotions. So, we decided to detect users physical interaction in three parameters including heart
                    rate,
                    touch pressure and rotation rate, which can identify user’s four emotions including calm, happy,
                    angry, energetic. Here, I focused on preparing the evaluation protocol, data collection and
                    documentation.
                </p>
                <div class="conbox">
                    <div class="fn-clear btnbox center">
                        <a href="image/protocol.pdf" class="btns">Evaluation Protocol</a>
                    </div>
                </div>
            </div>
        </div>

        <li class="line1"><span
                class="div">________________________________________________________________________</span></li>

        <div class="fn-clear">
            <iframe width="460px" height="280px" src="https://www.youtube.com/embed/jbOivnxUSYA"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            <div id="depo">
                <h3>Final Product</h3>
                <p>The final prototype was an iteration based on user evaluation. In this stage, we added GSR sensors to
                    measure skin electrical conductance and to detect heart rate. The music feedback was
                    improved that users could modulate music (tempo or melody) by changing the interaction approach, for
                    example, from touching to pressing the ball. We also used unity to control the visualization from
                    the projector based on users' emotion, so as to create more visual and playful feedback. Here, I was
                    responsible for video editing, documentation and project website development.
                </p>
                <div class="conbox">
                    <div class="fn-clear btnbox">
                        <a href="javascript:;" class="btns fl" onclick="myFunction5()">Show more</a>
                        <a href="image/proposal_TeamPop5.pdf" class="btns fl">Project Web</a>
                    </div>
                </div>
            </div>
        </div>

        <div id="myDIV5" style="display: none;">
            <li class="line"><span
                    class="div">________________________________________________________________________</span></li>

            <div class="cour">
                <p>User Interaction: While users interacted with MoodBall through touching, squeezing, shaking, rotating
                    and pressing, it would play relevant songs that reflect the user's current emotions. The remote ball
                    can
                    be connected with MoodBall and received the same music with lights and visual feedback.
                </p>
                <p>Form & Function: Emotion detection system of MoodBall include GSR sensor, pressure sensor and
                    accelerometer
                    gyroscope.
                    Accelerometer gyroscope was to measure the acceleration and rotation level. Two pressure sensors
                    detected the pressure forces. GSR
                    sensor was to detect skin electrical conductance (heart rate).</p>
                <img style="width: 50%; float: left;" src="image/5.jpg" alt="">
                <img style="width: 50%;" src="image/6.png" alt="">

                <p>
                    Music, lights and visualization: Speaker and projector were used to create an immersive music
                    playing
                    experience. Various background music, light colors and projector visualizations represent various
                    emotions.
                </p>
                <img style="width: 25%; float: left;;" src="image/22.jpg" alt="">
                <img style="width: 25%; float: left;" src="image/23.jpg" alt="">
                <img style="width: 25%; float: left;" src="image/24.jpg" alt="">
                <img style="width: 25%;" src="image/25.jpg" alt="">
                <img style="width: 100%;" src="image/table.png" alt="">

                <p><br>Calm: music (<a
                        href="https://drive.google.com/file/d/1U-tXaX0-599qbcA24oaKippze5VHSapn/view?usp=sharing">Calm</a>),
                    light (Blue), visual (<a
                        href="https://drive.google.com/file/d/1afR0raHKK2rUtKBCvt4sTmZiP_eipfk3/view?usp=sharing">Water
                        Wave</a>)<br>
                    Happy: music (<a
                        href="https://drive.google.com/file/d/1i65P6wGfvfg7kWgc3njrTENbq8Tv7QNO/view?usp=sharing">Happy</a>),
                    light (colorful RGB), visual (<a
                        href="https://drive.google.com/file/d/1mz4usV-7jtnKNi-UTsfDPgx7YLJdjo4L/view?usp=sharing">Colourful
                        Heart</a>)<br>
                    Angry: music (<a
                        href="https://drive.google.com/file/d/1IipdH8yN672dRidRGaUO2ZQdtFyhyYYE/view?usp=sharing">Angry</a>),
                    light (Red), visual (<a
                        href="https://drive.google.com/file/d/1rLrnuf4iznGsnx2Z8WGDTttJ07tg5oWJ/view?usp=sharing">Fire
                        Burning</a>)<br>
                    Energetic: music (<a
                        href="https://drive.google.com/file/d/1wRavYcDK80Ng3vVABXRu9g92XPz4UrgE/view?usp=sharing">Energetic</a>),
                    light (Green), visual (<a
                        href="https://drive.google.com/file/d/1FmGvQyJjFrmObg6otbXP27mrQmnaEfRP/view?usp=sharing">Green
                        Leaves</a>)
                </p>

                <h4>Link to the coding files:</h4>
                <p><a href="https://drive.google.com/file/d/1Z4hYMRmcyLnhaV6CTAfP433f2oVhB21B/view?usp=sharing">Pressure
                        Sensor (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/file/d/1EnrCqefUKrvnHHEZHka5gC68fNeE1vN5/view?usp=sharing">GSR
                        Sensor (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/file/d/11euxWk1ysAXznEVH3XXcl5okC-fiBWuh/view?usp=sharing">Accelerometer
                        Gyroscope (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/file/d/1K68gghdy9l0yG2a8z1dak6_MlpbkzJvv/view?usp=sharing">Combined
                        Sensors
                        With Formatted Outputs (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/file/d/1E-hZOpLEooi42mqY2JPCAHEHMu9v6yC-/view?usp=sharing">Unity
                        for
                        Receiving Formatted Outputs (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/file/d/1qLB2aA-WfPrbhjzjas02IeedyltUvX-K/view?usp=sharing">Unity
                        for
                        Switching Backgrounds (author/Thomas & Zhiyuan)</a><br>
                    <a href="https://drive.google.com/drive/folders/1i1rjwHqpWBfoSM7W8gD4xozyIl9vZisW?usp=sharing">Background
                        music (collect/Liqian You & Xiaodan Liu)</a><br>
                    <a href="https://youtu.be/bwEPWWMTuLs">Video prototype (editor/Liqian You)</a>
                </p>

                <h4>Link to other graphic design:</h4>
                <p><a href="https://drive.google.com/file/d/1J45H34N0QFQMF0lsYnyM-d0UA9OWBZUL/view?usp=sharing">Brochure
                        (author/Xiaodan Liu)</a><br>
                    <a href="https://drive.google.com/file/d/1pMEEVjPWFN564WiFaBktDQIE1xvGjbTz/view?usp=sharing">Poster
                        (author/Ziyi Xu)</a>
                </p>
            </div>
        </div>
    </section>

    <section id="outcome">
        <h2>-- Outcome --</h2>
        <p class="p1">During the course study, I have learned a lot. It was a pity that as an external student, I
            haven't joined in the final exhibit on site. However, I learned teamwork and remote cooperation through the
            online platform. I contributed my design and coding skills in completing several tasks in our team project,
            including 1. Make a plan to complete tasks on time; 2. Write several documents, presentation pitch, video
            voice and transcripts; 3. Find valuable samples and materials on websites to support techniques development;
            4. Multimedia support, project website design and development and video editing...</p>
        <p>Link to <a
                href="https://docs.google.com/document/d/18X5QiNCb2OCnTLPog84tlSZMZvF9dt4f3RtlEWZ5OEc/edit?usp=sharing">Critical
                Reflection</a></p>
    </section>

    <footer>
        <p>
            2021 DECO 7385 @Liqian YOU
            < Link to <a style="color: rgb(255, 255, 255);"
                href="https://drive.google.com/drive/folders/1SHhwUlF8yYDV4tuW4JzBiCIo6CiWxq8n?usp=sharing">Support
                Materials</a> >
                <a href="https://www.linkedin.com/in/liqian-you-257b00b4/"><img src="image/icin.svg" alt=""></a>
                <a href="https://www.facebook.com/liqian.you.5"><img src="image/icf.svg" alt=""></a>
                <a href="https://imgur.com/4xssKjN"><img src="image/icw.svg" alt=""></a>
                <a href="https://github.com/LiqianYOUY"><img src="image/icg.svg" alt=""></a>
        </p>
    </footer>

    <script>
        function move(data) {
            let scroll_offset = 0;
            if (data == 1) {
                scroll_offset = $('#concept').offset();

            } else if (data == 2) {
                scroll_offset = $('#function').offset();

            } else if (data == 3) {
                scroll_offset = $('#design').offset();

            } else if (data == 4) {
                scroll_offset = $('#outcome').offset();

            }
            $("body,html").animate({
                scrollTop: scroll_offset.top - 0
            })
        }

        function myFunction1() {
            var x = document.getElementById("myDIV1");
            if (x.style.display == "none") {
                x.style.display = "block";
            } else {
                x.style.display = "none";
            }
        }

        function myFunction2() {
            var m = document.getElementById("myDIV2");
            console.log(m.style.display)
            if (m.style.display == "none") {
                m.style.display = "block";
            } else {
                m.style.display = "none";
            }
        }

        function myFunction3() {
            var m = document.getElementById("myDIV3");
            console.log(m.style.display)
            if (m.style.display == "none") {
                m.style.display = "block";
            } else {
                m.style.display = "none";
            }
        }

        function myFunction4() {
            var m = document.getElementById("myDIV4");
            console.log(m.style.display)
            if (m.style.display == "none") {
                m.style.display = "block";
            } else {
                m.style.display = "none";
            }
        }

        function myFunction5() {
            var m = document.getElementById("myDIV5");
            console.log(m.style.display)
            if (m.style.display == "none") {
                m.style.display = "block";
            } else {
                m.style.display = "none";
            }
        }

        window.onscroll = function () {
            myFunction();
            console.log(window.pageYOffset)
        };

        var navbar = document.getElementById("menu");
        var sticky = navbar.offsetTop;
        var mainheight = document.getElementById("main").clientHeight;
        console.log(mainheight)


        function myFunction() {
            if (window.pageYOffset >= mainheight) {
                navbar.classList.add("sticky")
            } else {
                navbar.classList.remove("sticky");
            }
        }
    </script>
    <script src="canvas-nest.min.js"></script>

</body>

</html>