<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Google Font Source Sans Pro -->
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro" rel="stylesheet">

  <title>2021 : CONNECTION</title>

  <link rel="stylesheet" type="text/css" href="https://deco3850-ixd-exhibit.uqcloud.net/css/masterstyle.css">

  <!-- Rebox JS CSS -->
  <link rel="stylesheet" media="all" type="text/css"
    href="https://deco3850-ixd-exhibit.uqcloud.net/css/jquery-rebox.css" />


</head>

<body>
  <div class="container">
    <!-- Images + captions (nominate one as representative/thumbnail) – no limit on amount, will need to cater for both portrait & landscape format -->

    <!-- Representative Image/Thumbnail -->
    <header>

      <div class="title">
        <!-- Project Title & Team Name -->
        <h1 class="titletext">CONNECTION</h1>
        <h2 class="teamname">POP5</h2>

        <!-- Project 1-liner – max 140 characters. -->
        <h3 class="tagline">A music sharing & emotion detection ball that allows users to communicate moods through
          various physical interactions</h3>
      </div>
    </header>
    <div class="content">

      <aside class="left">

        <!-- Project theme & domain -->
        <div class="category">
          <div class="task">
            <h3><span class="label theme"></span></h3>
            <p>Modulating Music</p>
          </div>
          <div class="task">
            <h3><span class="label domain"></span></h3>
            <p>Share Emotion Through Music</p>
          </div>

          <!-- List of Team Members (using their preferred names), their key contributions/role and preferred contact information (email - don't use your s122345 email) -->
          <h3><span class="label teamtitle"></span></h3>
          <ul class="nodot team">
            <li class="member"><a href="mailto:zhutianyu.yu@uqconnect.edu.au">Zhutianyu Yu</a><br><span
                class="role">Hardware & Software Specialist, Arduino Programmer, Implementer</span></li>
            <li class="member"><a href="mailto:zhiyuan.zhang@uqconnect.edu.au">Zhiyuan Zhang</a><br><span
                class="role">Unity Expert, Arduino Programmer, Implementer, Hardware Specialist</span></li>
            <li class="member"><a href="mailto:ziyi.xu1@uqconnect.edu.au">Ziyi Xu</a><br><span class="role">User
                Researcher & Tester, Co-ordinator, Implementer</span></li>
            <li class="member"><a href="mailto:youyoucandlin@gmail.com">Liqian You</a><br><span class="role">Multimedia
                Support, UX Designer, Front-end Developer</span></li>
            <li class="member"><a href="mailto:xiaodan.liu@uqconnect.edu.au">Xiaodan Liu</a><br><span class="role">Web
                designer & Developer</span></li>
          </ul>

          <!-- List of interaction design methods used to inform project -->
          <h3><span class="label">Methods Used</span></h3>
          <ul class="nodot">
            <li class="method">Literature Review<br>Interview<br>User Scenarios<br>Bodystorming<br>Prototyping<br>Storyboard</li>
          </ul>

          <!-- List of additional keywords (these can be type of interaction, audience, technology… ) -->
          <h3><span class="label">Keywords</span></h3>
          <ul class="nodot">
            <li class="keyword">Physical Interaction<br>Emotion Detection<br>Music Sharing<br>Arduino<br>Unity</li>
          </ul>
          <!-- List of technology used in the implementation of your project. If you know this now, fill it in now. Otherwise this should be completed following the exhibit.
          <h3><span class="label">Technology Used</span></h3>
          <ul class="nodot">
            <li class="tech">Technology</li>
          </ul>
		  -->
        </div>

      </aside>

      <div class="right">

        <!-- Kickstarter style/explainer embedded video -->
        <!-- https://codepen.io/cobycreative/pen/cIrnv -->
        <div class="wrapper">
          <div class="h_iframe">
            <!-- copy YouTube embed code here! -->
          </div>
        </div>

        <!-- Project Description – 150 - 300 words describing what the project is and the intended experience. -->
        <p>CONNECTION is an emotion detection ball that allows users to detect and share feelings through music and
          lights. It aims to create immersive music modulating experience for joy and relaxation as well as to provide
          mental support through sharing and engaging with remote users. According to the user testing, a user who has
          different moods tends to interact with the ball differently, which includes touching, squeezing, rotating,
          etc. The ball can receive a user’s inputs reflected by his/her current moods and therefore changes tempo,
          pitch and melodies of the music as well as lights accordingly. Besides, it is able to connect to other devices
          such as the projector to create a more playful and charming experience and other remote balls to achieve the
          music sharing feature.</p>

        <p>The device requires users to interact with the ball in playful ways to play music, including touching,
          shaking, pressing, rotating, etc… When users touch it, it can detect the user's emotion through three ways
          (heart rate, pressure and rotation), and play relevant music with enchanting lights and visual feedback. Then,
          when users change another form of interaction (i.e. rotate the ball), different types of music will be played
          accordingly. The ball can receive the user's emotion data and change melodies or notes of the music based on
          the mood. It can also share music automatically with a remote ball. When the remote ball receives the data, it
          can play the same music with lights and visualization. We choose to express and share emotions in a more
          subtle way -- through music, instead of words, because music is another language of emotion, representing
          different feelings and barge into the soul without limits or restrictions. Introverts are usually ashamed to
          express themselves through words, but music is a good way of imitating their emotions. They can tell a story
          and share how they “really feel” through music, which allows more people to “understand them”.</p>

        <p>From record players to mobile apps, the approach of music sharing has been shifted from physically to
          virtually. Moreover, smart speakers have brought screen-free interaction, and the mental detection AI
          technology may help to sort out music based on users emotions, so that we can share our feelings effectively
          and efficiently through music. The future music streaming tends to be immersive, collaborative and artificial
          intelligence. In a fast-paced work environment, people have suffered more mental pressures than before. Those
          living alone, struggling with work and life would require more mental support in the social community. While
          nowadays, emotion detection devices with playful interaction are limited, we attempt to create a novel way of
          communicating moods through music and connect users with their families and friends in everyday life. We hope
          to use CONNECTION to bond social closeness physically and remotely, or just provide a little bit of
          psychological support in a negative mental condition.</p>

        <p>The view of "music not merely communicates basic emotions, but related to constructivist account" from
          Cespedes-Guevara[1] has inspired us to explore how music expresses core emotions and how to influence
          listeners' mental status. Our idea of developing concepts from the perspective of music expression and emotion
          detection was inspired by "Solo" Emotional Radio[2] that can recognize the user's facial expressions and play
          the music that matches the user's mood. The sensors used in an audiovisual installation[3] that can translate
          emotions into light inspired us to build the emotion detection function. The several interesting physical
          interaction methods of our device to convey user emotions were inspired by the mood reflecting floor that
          detects emotion by different body language such as fast and powerful movement.[4]</p>

        <h3>Technical Development</h3>
        <p>Emotion detection systems include GSR sensor, pressure sensor and accelerometer gyroscope, and each sensor’s
          level is based on the average rate from user testing responses. According to the users responses, we found
          that different users' interactions represent different emotions.</p>
        <p>Touch the ball: calm (5/9) <br>
          Shake the ball: energetic (5/9)<br>
          Throw the ball: angry (5/9)<br>
          Roll the ball: happy (5/9)<br></p>

        <p>Therefore, users' emotions are detected in three parameters including heart rate (GSR sensor), touch pressure
          (pressure sensor) and rotation rate (accelerometer gyroscope), and then play the corresponding music. These
          three parameters can identify the user's four emotions including calm, happy, angry, energetic. Each parameter
          is set at a certain level.</p>

        <p>Music and visual feedback: Speaker and projector are used to build an immersive music playing experience,
          different background music and different light colors represent different moods.</p>
        <p>Calm: music (), light (), visual ()<br>
          Happy: music (), light (), visual ()<br>
          Angry: music (), light (), visual ()<br>
          Energetic: music (), light (), visual ()</p>

        <h2>Link to the coding files and technique documents:</h2>
        <p>Pressure sensor<br>
          GSR sensor<br>
          Accelerometer gyroscope<br>
          Speaker<br>
          Light<br>
          Power supplier<br>
          Background music</p>


        <h3>Form & Function</h3>

        <p>1.Mood music: touch and play relevant songs based on your mood<br>
          2.Share emotion: share this song (positive or negative mental conditions) with others<br>
          3.Interaction: remote users receive audio and visual feedback<br>
          4.Modulate music: communicate feelings physically and remotely by using emotion detection technologies<br>
          5.Promptly receive other’s emotion through music, integrate emotions from sharer and listeners and provide
          emotional comfort/support (change melody or note) anywhere/anytime<br>
          6.Physical & Playful: interaction with touching, shaking, pressing, rotating, moving that creates a novel
          approach to share emotions<br>
        </p>

        <h3>Reference</h3>
        <p>[1] Cespedes-Guevara, J., & Eerola, T. (2018). Music Communicates Affects, Not Basic Emotions – A
          Constructionist Account of Attribution of Emotional Meanings to Music. Frontiers in Psychology, 9, 215.
          https://doi.org/10.3389/fpsyg.2018.00215<br>
          [2] Guardian News, Solo, the “emotional radio” that plays music to suit your mood. 2017.
          https://youtu.be/-8nVJ3Zz_SE<br>
          [3] Dezeen, Audiovisual installation translates emotions into beams of light. 2017.
          https://youtu.be/8FkBA3xTne0<br>
          [4] Visualcraft, The mood reflecting floor - Interaction design project. 2014. https://youtu.be/JOg4chJAgn8
        </p>

        <div id="gallery2" class="gallery image-container">

          <!--

          COPY & PASTE THIS FOR EACH NEW IMAGE YOU WISH TO DISPLAY. BE SURE TO REPLACE THE FULL SIZE IMAGE & THUMBNAIL WHERE APPROPRIATE (LOOK FOR THE CAPITALISED TEXT. AND DON'T FORGET YOUR CAPTIONS!!!!!!

          <div class="pictosquare">
            <a href="FULL SIZE OF IMAGE A" title="CAPTION FOR IMAGE A">
              <img class="containimage" src="THUMBNAIL FOR IMAGE A (CAN BE THE SAME URL AS ORIGINAL, BUT THUMBNAIL PREFERRED 400PX" alt="ALTERNATE TEXT FOR IMAGE A"/>
            </a>
          </div>

          -->

          <!-- FIRST IMAGE IS TO BE A FEATURE IMAGE -->
          <div class="pictosquare">
            <a href="images/ra.png" title="System Interaction Flow"><img class="containimage" src="images/ra.png"
                alt="System Interaction Flow" /></a>
          </div>

          <div class="pictosquare">
            <a href="images/1.jpg" title="User Scenario"><img class="containimage" src="images/1.jpg"
                alt="User Scenarios" /></a>
          </div>

          <div class="pictosquare">
            <a href="images/1.gif" title="Storyboard (Lo-fi Prototype)"><img class="containimage" src="images/1.gif"
                alt="User Scenarios"/></a>
          </div>
        </div>
        <!-- To be completed following the Exhibit - a description of how the project is built (physical form, technology hardware and software. And a final statement regarding the response from visitors to the exhibit and next steps for the project. 
        <h3>Technical Description</h3>
        <p>Maximum 500 words describing the implementation & technical specifications of your project.</p>
        <h3>Final Statement</h3>
        <p>Maximum 500 words describing the outcomes of the exhibit, public response to your work and key next steps for the project.</p>
        --->

      </div>
    </div>
  </div>

  <!-- Rebox JS Script -->
  <script src="https://deco3850-ixd-exhibit.uqcloud.net/lib/jquery.min.js"></script>
  <script src="https://deco3850-ixd-exhibit.uqcloud.net/lib/jquery-litelighter.js"></script>
  <script src="https://deco3850-ixd-exhibit.uqcloud.net/js/jquery-rebox.js"></script>
  <script src="https://deco3850-ixd-exhibit.uqcloud.net/js/imagefit.js"></script>
  <script>
    $('#gallery2').rebox({ selector: 'a' });
  </script>

</body>

</html>